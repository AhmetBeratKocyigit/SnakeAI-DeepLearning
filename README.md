# ğŸ Deep Q-Learning Snake AI

![Python](https://img.shields.io/badge/Python-3.8%2B-blue?style=for-the-badge&logo=python&logoColor=white)
![PyTorch](https://img.shields.io/badge/PyTorch-EE4C2C?style=for-the-badge&logo=pytorch&logoColor=white)
![Pygame](https://img.shields.io/badge/Pygame-333333?style=for-the-badge&logo=pygame&logoColor=white)
![License](https://img.shields.io/badge/License-MIT-green?style=for-the-badge)

> An Artificial Intelligence agent that learns to play the classic Snake game using **Deep Q-Networks (DQN)** and Reinforcement Learning.

---

## ğŸ“– Overview

This project implements an Artificial Intelligence (AI) agent that learns to play the classic Snake game using the **Deep Q-Network (DQN)** algorithm, a powerful technique in Reinforcement Learning (RL). The agent trains itself by maximizing rewards (eating apples) and minimizing penalties (crashing into walls or its own tail).

The AI uses a **Linear QNet** with **Experience Replay**, optimized to solve the "spinning loop" problem common in snake AI agents.

## âœ¨ Key Features

* **ğŸ§  Deep Q-Network (DQN):** Uses a Feed Forward Neural Network to predict the best action based on the game state.
* **ğŸ’¾ Experience Replay:** Stores past moves in memory to train on random batches, breaking correlation between consecutive steps.
* **ğŸ¯ Optimized Reward Shaping:**
    * Positive reward for moving towards food (0.1).
    * Heavy penalty for moving away from food (-0.5) or wasting time.
    * Critical penalty for death (-50).
* **ğŸ¤– Dual Modes:** Includes both a training mode (fast-paced learning) and a play mode (human-watchable speed).
* **ğŸ“Š State Persistence:** Automatically saves the model (`model.pth`) when a new high score is reached.

---

## ğŸ› ï¸ Installation

1.  **Clone the repository:**
    ```bash
    git clone [[https://github.com/yourusername/snake-ai-pytorch.git](https://github.com/yourusername/snake-ai-pytorch.git)](https://github.com/AhmetBeratKocyigit/SnakeAI-DeepLearning)
    cd SnakeAI-DeepLearning
    ```

2.  **Install dependencies:**
    It is recommended to use a virtual environment.
    ```bash
    pip install -r requirements.txt
    ```
    *(Dependencies: `pygame`, `torch`, `numpy`)*

---

## ğŸš€ How to Run

### 1. Training Mode (`main.py`)
Train the AI from scratch. You will see the agent improve over time.

```bash
python main.py
```

# ğŸ Snake AI â€“ Deep Q-Learning

This project uses Deep Q-Learning with PyTorch to train an AI that plays Snake.  
The training runs at high speed and the model is saved to:

```
./model/model.pth
```

---

## ğŸš€ 1. Training Mode (`main.py`)

The game runs extremely fast for efficient training.  
The model is automatically saved during training.

---

## ğŸ® 2. Demonstration Mode (`play_mode.py`)

Watch the trained model play Snake at normal speed:

```bash
python play_mode.py
```

**Controls**
- **R** â†’ Start / Restart  
- **ESC** â†’ Quit  

Exploration is disabled in this mode â†’ `epsilon = 0`.

---

## ğŸ“‚ Project Structure

```plaintext
â”œâ”€â”€ agent.py           # The Brain: Agent class, Q-Network, and training logic
â”œâ”€â”€ snake_game_ai.py   # The Body: Pygame environment, collision detection, UI
â”œâ”€â”€ main.py            # The Trainer: Main training loop
â”œâ”€â”€ play_mode.py       # The Player: Loads and runs the trained model
â”œâ”€â”€ model/             # Contains saved models (model.pth)
â””â”€â”€ README.md          # Documentation
```

---

## âš™ï¸ Hyperparameters & Configuration

Located in `agent.py`:

| Parameter      | Value     | Description |
|----------------|-----------|-------------|
| Batch Size     | 1000      | Number of samples drawn from memory per update |
| Learning Rate  | 0.001     | Rate at which network weights are updated |
| Gamma          | 0.95      | Discount factor for future rewards |
| Memory Size    | 100,000   | Maximum replay memory size |
| Epsilon Floor  | 10â»Â²â°     | Minimum randomness to avoid agent loops |

---

## ğŸ¤ Contributing

Contributions are welcome!  
Ideas such as improved reward functions or a CNN-based full-screen agent are appreciated.

1. Fork the project  
2. Create your feature branch â†’ `git checkout -b feature/AmazingFeature`  
3. Commit your changes â†’ `git commit -m "Add AmazingFeature"`  
4. Push to your branch â†’ `git push origin feature/AmazingFeature`  
5. Open a Pull Request  

---

## ğŸ“ License

Distributed under the **MIT License**.  
See the LICENSE file for more information.

---

**Developed with â¤ï¸ using PyTorch and Pygame.**



